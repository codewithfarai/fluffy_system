version: '3.8'

services:
  data-processor:
    image: python:3.11-alpine
    command: |
      sh -c "
        pip install redis celery psycopg2-binary sqlalchemy
        cat > processor.py << 'EOF'
        import os
        import time
        import json
        import logging
        from celery import Celery
        from sqlalchemy import create_engine, text
        import redis
        
        # Configure logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        # Initialize Celery
        redis_url = f\"redis://:{os.getenv('REDIS_PASSWORD', '')}@{os.getenv('REDIS_HOST', 'redis')}:{os.getenv('REDIS_PORT', 6379)}/0\"
        app = Celery('data-processor', broker=redis_url, backend=redis_url)
        
        # Database connection
        db_url = f\"postgresql://{os.getenv('DB_USER', 'app_user')}:{os.getenv('DB_PASSWORD', '')}@{os.getenv('DB_HOST', 'postgres')}:{os.getenv('DB_PORT', 5432)}/{os.getenv('DB_NAME', 'app_db')}\"
        
        @app.task
        def process_data(data_id, data_payload):
            \"\"\"Process data asynchronously\"\"\"
            logger.info(f'Processing data ID: {data_id}')
            
            try:
                # Simulate data processing
                time.sleep(5)
                
                # Store result in database
                engine = create_engine(db_url)
                with engine.connect() as conn:
                    result = conn.execute(text(\"SELECT version()\"))
                    db_version = result.fetchone()[0]
                    logger.info(f'Connected to database: {db_version}')
                
                # Store result in Redis
                r = redis.Redis.from_url(redis_url)
                result_key = f'processed:{data_id}'
                result_data = {
                    'id': data_id,
                    'status': 'completed',
                    'processed_at': time.time(),
                    'payload': data_payload
                }
                r.setex(result_key, 3600, json.dumps(result_data))
                
                logger.info(f'Data processing completed for ID: {data_id}')
                return {'status': 'success', 'data_id': data_id}
                
            except Exception as e:
                logger.error(f'Error processing data ID {data_id}: {str(e)}')
                return {'status': 'error', 'data_id': data_id, 'error': str(e)}
        
        @app.task
        def cleanup_old_data():
            \"\"\"Clean up old processed data\"\"\"
            logger.info('Starting cleanup of old data')
            
            try:
                r = redis.Redis.from_url(redis_url)
                keys = r.keys('processed:*')
                
                cleaned_count = 0
                for key in keys:
                    data = json.loads(r.get(key))
                    if time.time() - data['processed_at'] > 86400:  # 24 hours
                        r.delete(key)
                        cleaned_count += 1
                
                logger.info(f'Cleaned up {cleaned_count} old data entries')
                return {'status': 'success', 'cleaned_count': cleaned_count}
                
            except Exception as e:
                logger.error(f'Error during cleanup: {str(e)}')
                return {'status': 'error', 'error': str(e)}
        
        if __name__ == '__main__':
            app.worker_main(['worker', '--loglevel=info', '--concurrency=4'])
        EOF
        
        python processor.py
      "
    environment:
      REDIS_HOST: database_redis
      REDIS_PORT: 6379
      DB_HOST: database_postgres
      DB_PORT: 5432
      DB_NAME: app_db
      DB_USER: app_user
      CELERY_BROKER_URL: "redis://:${REDIS_PASSWORD}@database_redis:6379/0"
      CELERY_RESULT_BACKEND: "redis://:${REDIS_PASSWORD}@database_redis:6379/0"
    secrets:
      - source: db_password
        target: DB_PASSWORD
      - source: redis_password
        target: REDIS_PASSWORD
    networks:
      - worker-network
      - database-network
    deploy:
      mode: replicated
      replicas: 3
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
        reservations:
          memory: 256M
          cpus: "0.5"
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3

  scheduler:
    image: python:3.11-alpine
    command: |
      sh -c "
        pip install redis celery psycopg2-binary celery-beat
        cat > scheduler.py << 'EOF'
        import os
        from celery import Celery
        from celery.schedules import crontab
        
        # Initialize Celery
        redis_url = f\"redis://:{os.getenv('REDIS_PASSWORD', '')}@{os.getenv('REDIS_HOST', 'redis')}:{os.getenv('REDIS_PORT', 6379)}/0\"
        app = Celery('scheduler', broker=redis_url, backend=redis_url)
        
        # Schedule configuration
        app.conf.beat_schedule = {
            'cleanup-every-hour': {
                'task': 'processor.cleanup_old_data',
                'schedule': crontab(minute=0),  # Run every hour
            },
            'health-check': {
                'task': 'processor.health_check',
                'schedule': 300.0,  # Run every 5 minutes
            },
        }
        
        app.conf.timezone = 'UTC'
        
        if __name__ == '__main__':
            app.start(['celery', 'beat', '--loglevel=info'])
        EOF
        
        python scheduler.py
      "
    environment:
      REDIS_HOST: database_redis
      REDIS_PORT: 6379
      DB_HOST: database_postgres
      DB_PORT: 5432
      DB_NAME: app_db
      DB_USER: app_user
    secrets:
      - source: redis_password
        target: REDIS_PASSWORD
    networks:
      - worker-network
      - database-network
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.25"
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3

  monitor:
    image: python:3.11-alpine
    command: |
      sh -c "
        pip install redis celery psycopg2-binary flower
        cat > monitor.py << 'EOF'
        import os
        from celery import Celery
        
        # Initialize Celery for monitoring
        redis_url = f\"redis://:{os.getenv('REDIS_PASSWORD', '')}@{os.getenv('REDIS_HOST', 'redis')}:{os.getenv('REDIS_PORT', 6379)}/0\"
        app = Celery('monitor', broker=redis_url, backend=redis_url)
        
        # Start Flower monitoring
        os.system(f'celery -A monitor flower --port=5555 --broker={redis_url}')
        EOF
        
        python monitor.py
      "
    environment:
      REDIS_HOST: database_redis
      REDIS_PORT: 6379
    secrets:
      - source: redis_password
        target: REDIS_PASSWORD
    ports:
      - "5555"
    networks:
      - worker-network
      - database-network
      - traefik-public
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.25"
      labels:
        - traefik.enable=true
        - traefik.http.routers.flower.rule=Host(`flower.example.com`)
        - traefik.http.routers.flower.entrypoints=websecure
        - traefik.http.routers.flower.tls.certresolver=letsencrypt
        - traefik.http.services.flower.loadbalancer.server.port=5555
        - traefik.http.routers.flower.middlewares=flower-auth
        - traefik.http.middlewares.flower-auth.basicauth.users=admin:$$2y$$10$$example_hash

secrets:
  db_password:
    external: true
  redis_password:
    external: true

networks:
  worker-network:
    driver: overlay
    attachable: true

  database-network:
    external: true

  traefik-public:
    external: true